# define input texts

articles = [
"""    President Joe Biden announced his administration’s long-awaited student loan forgiveness plan Wednesday, saying it will forgive $10,000 in student loans for borrowers who earned less than $125,000 during the pandemic. People who received Pell Grants, grants to low-income students, while they were enrolled in college will be eligible to have $20,000 in debt forgiven.

The move will be enough to wipe out some student debt entirely: 15 million of the 43 million people with federal loans owe less than $10,000, and those borrowers are typically the most likely to fail to pay their loans back. In all, the plan will eliminate student debt for about 20 million people, according to an analysis provided by the Education Department, and decrease monthly payments by an average of $250 for borrowers with a remaining balance who are on standard 10-year payment plans.

The Committee for a Responsible Federal Budget, an anti-deficit group, came up with a price tag of $230 billion for a less-generous version of the program that did not include the additional aid for Pell Grant recipients.

Biden’s announcement came about after months of speculation and a pause on student loan payments that lasted more than two years. Ultimately, the plan showcases the attraction, and the limitation, of executive action. Biden had the opportunity to provide immediate financial relief to millions in need, and he took it. But the challenge of preventing people from falling into the student debt trap in the future remains.

Here’s what we know about the policy and how it would work.

I have student loans. What does the announcement mean for me?
If you have a federal “Direct Loan” — the most common type of student loan — issued before June 30, 2022, you will be able to apply to have your outstanding balance reduced. All direct loans are eligible, including loans to parents and graduate students.

If you qualify and your balance is less than $10,000, the loan will be retired; if you received a Pell Grant while enrolled in college, the amount that can be forgiven goes up to $20,000.

Student loan borrowers gather near The White House on May 12, calling for President Biden to cancel student debt. Paul Morigi/Getty Images for We, The 45 Million
Only people who earned less than $125,000 as an individual or $250,000 as part of a married couple in 2020 or 2021 will be eligible for forgiveness.
""",

"""    A brand-new Model 3 delivered in desperate need of repair, due to a faulty computer, an inoperable wireless phone charger, and a missing USB port. A supposedly fixed Tesla returned unfixed, and with something spilled on the car, damaging the paint. A dead mouse and rat poison discovered in a Tesla’s front trunk after a trip to the local service center.

These are just a few of the issues about Tesla service that customers have flagged in complaints filed with the Federal Trade Commission (FTC). Through a public records request, Recode obtained details about more than 1,000 complaints about Tesla, including more than 120 customer reports that discussed specific problems with service, delays, and parts. These reports reflect a growing frustration with the company over its approach to maintenance and repairs — discontent that’s echoed everywhere from Tesla owner Reddit posts to online forums for Tesla shareholders to the more than 9,000 reports that the company’s customers have sent to the Better Business Bureau. The complaints point to all sorts of problems with the experience of owning a Tesla vehicle, including an inadequate number of service centers, limited stock of replacement parts, bad communication, poor manufacturing quality, and long wait times for repair appointments.

Part of the frustration is that EVs are supposed to be easier to maintain than internal combustion vehicles. They don’t need oil changes, have fewer moving parts, and use regenerative braking, which means brake pads last longer. Tesla even says that it “designs every Tesla vehicle with the goal of eliminating the need for service.” Among car owners who do need service, however, Tesla drivers tend to visit service centers at nearly the same rate as the owners of premium gas-powered vehicles, such as Lexus or Audi, according to data the consumer research firm J.D. Power shared with Recode.

Tesla’s approach to service is also frustrating some customers. While automakers like Ford and GM enlist a network of thousands of dealerships and independent mechanics to fix their vehicles, it appears the vast majority of Tesla repairs tend to be completed by Tesla technicians employed or authorized by the company. Tesla warns customers that damage or failures caused by technicians who aren’t Tesla-certified won’t be covered by the company’s warranty policy. In the past, Tesla has also opposed legislation that supports the right-to-repair movement, which advocates for making it easier for people to repair their own devices and equipment.

Meanwhile, Tesla’s manufacturing output has historically increased at a significantly higher rate than the number of Tesla service centers, which, again, the vast majority of current Tesla owners rely on. The number of cars Tesla produced grew 68 percent in the first quarter of this year, compared to the same quarter last year, but the number of store and service locations grew by only 20 percent. Meanwhile, Tesla’s mobile service fleet — this is made up of Tesla mechanics that travel to a location of the customer’s choosing to complete repairs — grew by 35 percent. During the company’s annual stockholders meeting earlier this month, some Tesla investors questioned when the company would increase its current service capacity, which they say can’t support the number of Teslas on the road.

“Service has been a problem, but it has not affected sales in the least bit,” Rich Benoit, who runs a YouTube channel focused on Tesla repairs, explained. “So Tesla will not focus on this service issue until they see a decline in sales. So this really isn’t a problem for them.”


Recode reached out to Tesla with a request for comment about these complaints and to ask about the state of Tesla service. The company, which disbanded its public relations team in 2020, did not respond.
""",

"""   Remarking on the temperature is often cited as an example of boring conversation, but quite possibly the least awful effect of climate change is the fact that talking about the weather is now extremely interesting. In fact, often the weather is all I want to talk about, because aside from being a sinister reminder of the terror humans have wreaked on our home, it is, on rare occasions, also mysterious and wonderful. But it hardly matters what the weather is, just like it hardly matters what you watched on TV last night or what you did last weekend, because that isn’t the point of small talk.

There’s been a strong cultural pushback against small talk in recent years, or at least the idea of it. A cursory search on Twitter indicates that at least once per day, someone goes minorly viral tweeting about how much they hate small talk. It’s a common marketing strategy on dating apps (e.g., “let’s skip the small talk and go straight to the deep stuff”). This very website even published a piece by a staunch small-talk hater about why small talk is so “excruciating.”

i hate small talk. i want to hear about your childhood, your favorite scents, what types of music you like, your religious views, where you want to live when you get older, what keeps you up at night, how much certain things mean to you, your insecurities & fears.

— . (@httpsnimroood) July 28, 2022
To a certain extent, the haters have a point. You can only endure so many Tinder back-and-forths that start and end with “How was your weekend?” until you declare the act of small talk completely pointless. Perhaps you, like many of us, dread the moment you’re asked something about what you’re planning to do for the rest of the summer and your brain decides to take a vacation. Or you might experience the other problem, in which you, out of nerves or masochism or too many cocktails, spill your deepest secrets to a work acquaintance. (I have never done this!)

That combination of performance anxiety and the perceived meaningless of it all is a powerful repellent. But however painful it may be to some, small talk — which can be defined as light, polite conversation about trivial topics, either as a means of opening or closing a social interaction or simply to fill silence — has a point beyond surface-level time-wasting and cookie-cutter DMs. Meredith Marra, a professor of linguistics at the Victoria University of Wellington in New Zealand, has studied the social function of small talk for decades, and says that the very people who complain about it don’t realize how often they use it in their everyday lives. “There are some very masculine workplaces where if you don’t do small talk, everything stops,” she explains. “We can’t just go straight into work because we haven’t established our relationship yet. And if we don’t establish that we’re on the same page, how on earth are we going to get the other stuff done?”

She asserts that small talk is more than just avoiding awkward silence; it’s social glue. “Otherwise you’re just two people, not two people that are connected and trying to do the same thing,” she says.

It’s not new news that small talk makes us happier: In 2014, University of Chicago psychologist and scholar Nicholas Epley conducted an experiment in which some subway commuters were told to spark a conversation with the stranger sitting next to them while others were told to keep to themselves. Those who engaged in conversation enjoyed the ride more, and the longer the conversation, the happier they felt. This was true even for people who generally preferred solitude: “Those who misunderstand the consequences of social interactions may not, in at least some contexts, be social enough for their own well-being,” writes Epley.
""",

"""   EA started as an effort to get financially comfortable people in the UK and US to donate more money to charities that could do the most good, but it has rapidly evolved into much more: a source of career advice, a political movement, and the preferred cause of at least two mega-billionaire households, Cari Tuna and Dustin Moskovitz (net worth $14.2 billion) and Sam Bankman-Fried (net worth $13.3 billion).

The piece got a lot of feedback, most of it helpful, some of it less helpful. But most of the critical feedback to the story, and similarly themed ones from Gideon Lewis-Kraus in the New Yorker and Naina Bajekal in Time, boiled down to a simple argument: these rich donors need to pay more in taxes.

It’s a fair point. The EA of 2013, which was mostly a few philosophy professors and students donating 10 percent of their money to buy anti-malaria bednets, was not particularly entangled with the American billionaire class. Nearly a decade later, it very much is. Some people hate the billionaire class for very understandable reasons, and that enmity is inevitably going to rebound on EA.

The enmity is particularly pronounced for Bankman-Fried, given the fact that crypto is, let’s be blunt, a completely useless asset class that has serious environmental costs.

(Disclosure: Bankman-Fried’s family foundation, Building a Stronger Future, is funding some of the Future Perfect section at Vox, so let me bite the hand that feeds me and say that I think him buying up Super Bowl ads and Vogue spreads with Gisele Bündchen to encourage ordinary people to put their money into this pile of mathematically complex garbage is … actually morally questionable. Bankman-Fried can do a lot of good with the money FTX produces, but parts of the production process make me increasingly uncomfortable.)

The EA push for higher taxes on the rich
But the critique of EA philanthropy is also made in ways that are frankly just inaccurate. “A huge number of effective altruists support unconditional cash transfers,” the writer/podcaster Michael Hobbes tweeted, “but if you propose taxing them and funding the welfare state (i.e. the same thing) they lose their minds.”

I have obviously not been present for all of Hobbes’s conversations with every person who identified as an effective altruist, but in my experience this statement is bizarrely false.
"""
]

papers = [
""" Abstract: This scientific paper propose a novel portfolio optimization model using an improved deep reinforcement learning
algorithm. The objective function of the optimization model is the weighted sum of the expectation and value at risk(VaR)
of portfolio cumulative return. The proposed algorithm is based on actor-critic architecture, in which the main task of critical
network is to learn the distribution of portfolio cumulative return using quantile regression, and actor network outputs the
optimal portfolio weight by maximizing the objective function mentioned above. Meanwhile, we exploit a linear transformation
function to realize asset short selling. Finally, A multi-process method is used, called Ape-x, to accelerate the speed of deep
reinforcement learning training. To validate our proposed approach, we conduct backtesting for two representative portfolios
and observe that the proposed model in this work is superior to the benchmark strategies.

Keywords: Deep reinforcement learning; Algorithmic trading; Actor-critic architecture; Trading strategy

1 Introduction

Algorithmic trading, which has been widely used in the financial market, is a technique that uses a computer program to automate
the process of buying and selling stocks, options, futures and crypto currency, etc. Institutional investors such as pension funds,
mutual funds and hedge funds usually use algorithmic trading to find the most favorable execution price, so as to reduce the
impact of severe market fluctuations and improve execution efficiency. Algorithmic trading has a far-reaching impact on the
overall efficiency and microstructure of the capital market. Therefore, asset pricing, portfolio investment and risk measurement
may undergo revolutionary changes.

Classical algorithmic strategies includes arrival price strategy(Perold, 1988; Almgren & Lorenz, 2007); volume weighted
average price strategy(Berkowitz, Logue, & Noser Jr, 1988; Madhavan & Panchapagesan, 2002); time weighted average price
strategy(Kolm & Maclin, 2011), implementation shortfall strategy(Kritzman, 2006; Hendershott & Riordan, 2013); guerrilla
strategy(Eriksson & Swartling, 2012),etc. In recent years, with the rapid development of artificial intelligence(AI), more and
more researchers begins to utilize deep reinforcement learning to implement algorithmic trading. Deep reinforcement learning
mainly uses the excellent feature representation ability of deep neural network to fit the state, action, value and other functions
of reinforcement learning, so as to maximize portfolio return.

Currently, researchers exploited different reinforcement learning algorithms to study financial trading problems, including
actor-only method(also called value based method) critic only method(also called policy based method) and actor-critic method.
Nevertheless, we believe that these studies are not yet mature in term of practical application for following reason. First, most
studies maximize the average cumulative rewards, which is also called expectation of cumulative rewards, of a single asset or
a portfolio by training value function(such as policy gradient) or action-value function(such as Q-learning). However, these
algorithms does not take into account the risk and are only suitable for investors as neutralism risk type. A large number of
studies(Jondeau & Rockinger, 2003; Tsiakas, 2006; Franke, 2009) have proved that the stock return has the fat tail characteristics,
so the low-probability tail risk need to be highly concerned.

Additionally, short selling is allowed in many stock markets. Short selling can not only make profit in a bear market but
also reduce the speculation and volatility of the stock market. Although (Park, Sim, & Choi, 2020) and (Almahdi & Yang,
2019) have considered short selling, these studies focus on single asset trading. Most traders generally hold multiple securities,
unfortunately, most of the existing studies do not consider the short selling problem in this common case.
Finally, the interaction between agent and environment is often very time-consuming. However, profit opportunities are
fleeting in the stock market. Therefore, how to improve the training speed is also worth further study.
The main contribution of this paper is as follows:

Firstly, this study proposes a new algorithm, called
""",

"""Abstract

When inferring reward functions from human behavior (be it demonstrations,
comparisons, physical corrections, or e-stops), it has proven useful to model the
human as making noisy-rational choices, with a "rationality coefficient" capturing
how much noise or entropy we expect to see in the human behavior. Many existing
works have opted to fix this coefficient regardless of the type, or quality, of human
feedback. However, in some settings, giving a demonstration may be much more
difficult than answering a comparison query. In this case, we should expect to see
more noise or suboptimality in demonstrations than in comparisons, and should
interpret the feedback accordingly. In this work, we advocate that grounding the
rationality coefficient in real data for each feedback type, rather than assuming a
default value, has a significant positive effect on reward learning. We test this in
experiments with both simulated feedback, as well a user study. We find that when
learning from a single feedback type, overestimating human rationality can have
dire effects on reward accuracy and regret. Further, we find that the rationality level
affects the informativeness of each feedback type: surprisingly, demonstrations
are not always the most informative—when the human acts very suboptimally,
comparisons actually become more informative, even when the rationality level is
the same for both. Moreover, when the robot gets to decide which feedback type to
ask for, it gets a large advantage from accurately modeling the rationality level of
each type. Ultimately, our results emphasize the importance of paying attention to
the assumed rationality level, not only when learning from a single feedback type,
but especially when agents actively learn from multiple feedback types.

1 Introduction

Reward learning started from the inverse optimal control idea that we can recover the underlying
objective when observing optimal behavior [28], and transitioned into AI with the introduction of
inverse reinforcement learning [35]. While initial research assumed optimal demonstrators [35, 38],
the field quickly moved to the noisy-rational human model [34]: a number of simultaneous works,
with different motivations, converged on a Bolzmann (maximum entropy) distribution, where the
human actions are (exponentially) more probable the higher value they are [3, 45, 24, 42, 31, 29, 44,
7, 13, 17, 32]. Often this model would have a "rationality" coefficient β2 meant to capture how good
of an optimizer the human is—setting β to 0 would yield the uniform distribution capturing a random
human, while β → ∞ would put all the probability mass on optimal actions.

Inspired by the way economists look at preferences, the field then started looking beyond learning
from demonstrations to learning from comparisons [43, 13, 4]. The model was similar: still a Boltzmann distribution, but over two trajectories/actions, instead of over all possible trajectories/actions.
Other researchers started looking at a deluge of feedback types: comparisons [43], language [33],
demonstrations [35], preference rankings [8], corrections [2], critiques [14], e-stops [22], binary
feedback [30], and proxy rewards [23]. It turned out all of these can be interpreted as noisy-rational
(Boltzmann) choices [26], opening the door to learning from all of these in combination, and even
enabling robots to actively select what feedback type to ask for.

Boltzmann-rationality’s ability to unify different feedback types is useful, but the model comes with
this one parameter—β, which begs the question: what should we set that to? Prior work often either
omits β (implicitly setting it to 1) [17, 13, 25] or sets it to a fixed, often heuristic, value across all
feedback types [37, 26, 39, 4]. But demonstrations are sometimes easier or harder to give, depending
on the task and the interface, suggesting that β should be adapted to the domain. And comparisons
might be much easier to answer than demonstrations, suggesting we should be using a higher β for
the former. Our goal in this work is to answer the question: does this matter? Are there real benefits
to grounding β in real data for each feedback type, or is it safe to stick to a default value?

We analyze this in both simulation and a user study. First, we find that for each feedback type,
having a good estimate of β is useful for reward learning, and that overestimating the human’s
rationality is particularly harmful. Second, we find that estimating β is useful even when the data is
not produced by Boltzmann-rational demonstrators, but rather by much more realistic demonstrators
that have systematic biases. For example, myopic behavior can be somewhat effectively modeled as
noisy-rational with a lower β, still leading to well-performing reward inference. User study results
with real human biased behavior also support this finding
""",

"""Abstract.

Causal reasoning provides a language to ask important interventional and counterfactual questions beyond purely statistical association. In medical imaging, for example, we may want to study the
causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. However, while
anatomical shape models of 3D surface meshes, extracted from automated image segmentation, can be reliably constructed, there is a lack
of computational tooling to enable causal reasoning about morphological variations. To tackle this problem, we propose deep structural causal
shape models (CSMs), which utilise high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of
deep structural causal models. CSMs enable subject-specific prognoses
through counterfactual mesh generation (“How would this patient’s brain
structure change if they were ten years older?”), which is in contrast to
most current works on purely population-level statistical shape modelling. We demonstrate the capabilities of CSMs at all levels of Pearl’s
causal hierarchy through a number of qualitative and quantitative experiments leveraging a large dataset of 3D brain structures.
Keywords: Causality, geometric deep learning, 3D shape models, counterfactuals, medical imaging

1 Introduction

The causal modelling of non-Euclidean structures is a problem which machine
learning research has yet to tackle. Thus far, state-of-the-art causal structure
learning frameworks, utilising deep learning components, have been employed
to model the data generation process of 2D images [54 ,41 ,65, 76]. However, most
of these approaches fall short of answering counterfactual questions on observed
data, and to the best of our knowledge, none have been applied to non-Euclidean
data such as 3D surface meshes. Parallel streams of research have rapidly advanced the field of geometric deep learning, producing highly performant predictive and generative models of graphs and meshes [12,11,26,32,79,74]. Our work
finds itself at the intersection of these cutting-edge fields; our overarching contribution is a deep structural causal shape model (CSM), utilising geometric
deep learning components, of the data generation process of 3D meshes, with
tractable counterfactual mesh generation capabilities.

Traditional statistical learning techniques only allow us to answer questions
that are inherently associative in nature. For example, in a supervised learning
setting, we are presented with a dataset of independently and identically distributed features, from which we pick some to be the inputs x, in order to learn
a mapping to targets y using an appropriate machine learning model, such as a
neural network. It need not be the case however, that x caused y for their statistical relationship to be learned to high levels of predictive accuracy. In order
to answer many real-world questions, which are often interventional or counterfactual, we must consider the direction of causality between the variables in our
data. This is particularly important in the context of medical imaging [15], for
example, where one may need to compare the effect of a disease on the entire
population against the effect on an individual with characteristic genes, or other
important influencing factors, with all assumptions available for scrutiny.

Pearl’s causal hierarchy [55,29] is a framework for categorising statistical
models by the questions that they can answer. At the first level in the hierarchy,
associational models learn statistical relationships between observed variables,
e.g. p(y|x). In the case of deep generative modelling, for example, variational
autoencoders (VAE) [40], normalising flows (NF) [62,52] and generative adversarial networks (GAN) [31] all learn correlations between a latent variable and
the high-dimensional variable of interest, either implicitly or explicitly. Similarly,
in geometric deep learning, 3D morphable models [26,58,10,30,70,80,17,8,18] associate a latent vector to a 3D shape. At the next level, interventions can be
performed on the assumed data generating process, to simulate an event at the
population-level, by fixing the output of a variable generating function [63]. In
the simplest case, this amounts to interpolating an independent, latent dimension
in a statistical shape model [38,35,16], with prior work to semantically interpret
each latent dimension. Structural causal models (SCMs) (Section 3.1) can also
be built using neural networks to enable this functionality [76,41,65]. At the final level, counterfactual questions can be answered by using SCMs to simulate
subject-specific, retrospective, hypothetical scenarios [29]. Although some work
has been done in this field [41,65], Pawlowski et al.’s deep structural causal models (DSCM) framework [54] is the only one to provide a full recipe for tractable
counterfactual inference for high and low-dimensional variables, to the best of
our knowledge. However, they only considered 2D Euclidean data (images).
"""
]

code = [
"""
// SPDX-License-Identifier: GPL-2.0-or-later
/*
 *	AARP:		An implementation of the AppleTalk AARP protocol for
 *			Ethernet 'ELAP'.
 *
 *		Alan Cox  <Alan.Cox@linux.org>
 *
 *	This doesn't fit cleanly with the IP arp. Potentially we can use
 *	the generic neighbour discovery code to clean this up.
 *
 *	FIXME:
 *		We ought to handle the retransmits with a single list and a
 *	separate fast timer for when it is needed.
 *		Use neighbour discovery code.
 *		Token Ring Support.
 *
 *	References:
 *		Inside AppleTalk (2nd Ed).
 *	Fixes:
 *		Jaume Grau	-	flush caches on AARP_PROBE
 *		Rob Newberry	-	Added proxy AARP and AARP proc fs,
 *					moved probing from DDP module.
 *		Arnaldo C. Melo -	don't mangle rx packets
 */

#include <linux/if_arp.h>
#include <linux/slab.h>
#include <net/sock.h>
#include <net/datalink.h>
#include <net/psnap.h>
#include <linux/atalk.h>
#include <linux/delay.h>
#include <linux/init.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h>
#include <linux/export.h>
#include <linux/etherdevice.h>

int sysctl_aarp_expiry_time = AARP_EXPIRY_TIME;
int sysctl_aarp_tick_time = AARP_TICK_TIME;
int sysctl_aarp_retransmit_limit = AARP_RETRANSMIT_LIMIT;
int sysctl_aarp_resolve_time = AARP_RESOLVE_TIME;

/* Lists of aarp entries */
/**
 *	struct aarp_entry - AARP entry
 *	@last_sent: Last time we xmitted the aarp request
 *	@packet_queue: Queue of frames wait for resolution
 *	@status: Used for proxy AARP
 *	@expires_at: Entry expiry time
 *	@target_addr: DDP Address
 *	@dev:  Device to use
 *	@hwaddr:  Physical i/f address of target/router
 *	@xmit_count:  When this hits 10 we give up
 *	@next: Next entry in chain
 */
struct aarp_entry {
	/* These first two are only used for unresolved entries */
	unsigned long		last_sent;
	struct sk_buff_head	packet_queue;
	int			status;
	unsigned long		expires_at;
	struct atalk_addr	target_addr;
	struct net_device	*dev;
	char			hwaddr[ETH_ALEN];
	unsigned short		xmit_count;
	struct aarp_entry	*next;
};
""",
"""
// SPDX-License-Identifier: GPL-2.0-or-later
/*
 * af_alg: User-space algorithm interface
 *
 * This file provides the user-space API for algorithms.
 *
 * Copyright (c) 2010 Herbert Xu <herbert@gondor.apana.org.au>
 */

#include <linux/atomic.h>
#include <crypto/if_alg.h>
#include <linux/crypto.h>
#include <linux/init.h>
#include <linux/kernel.h>
#include <linux/list.h>
#include <linux/module.h>
#include <linux/net.h>
#include <linux/rwsem.h>
#include <linux/sched.h>
#include <linux/sched/signal.h>
#include <linux/security.h>

struct alg_type_list {
	const struct af_alg_type *type;
	struct list_head list;
};

static struct proto alg_proto = {
	.name			= "ALG",
	.owner			= THIS_MODULE,
	.obj_size		= sizeof(struct alg_sock),
};

static LIST_HEAD(alg_types);
static DECLARE_RWSEM(alg_types_sem);

static const struct af_alg_type *alg_get_type(const char *name)
{
	const struct af_alg_type *type = ERR_PTR(-ENOENT);
	struct alg_type_list *node;

	down_read(&alg_types_sem);
	list_for_each_entry(node, &alg_types, list) {
		if (strcmp(node->type->name, name))
			continue;

		if (try_module_get(node->type->owner))
			type = node->type;
		break;
	}
	up_read(&alg_types_sem);

	return type;
}

int af_alg_register_type(const struct af_alg_type *type)
{
	struct alg_type_list *node;
	int err = -EEXIST;

	down_write(&alg_types_sem);
	list_for_each_entry(node, &alg_types, list) {
		if (!strcmp(node->type->name, type->name))
			goto unlock;
	}

	node = kmalloc(sizeof(*node), GFP_KERNEL);
	err = -ENOMEM;
	if (!node)
		goto unlock;

	type->ops->owner = THIS_MODULE;
	if (type->ops_nokey)
		type->ops_nokey->owner = THIS_MODULE;
	node->type = type;
	list_add(&node->list, &alg_types);
	err = 0;
""",

"""
/**
 * @license
 * Copyright Google LLC All Rights Reserved.
 *
 * Use of this source code is governed by an MIT-style license that can be
 * found in the LICENSE file at https://angular.io/license
 */

import {InjectionToken} from '../di/injection_token';
import {isLView} from '../render3/interfaces/type_checks';
import {RENDERER} from '../render3/interfaces/view';
import {getCurrentTNode, getLView} from '../render3/state';
import {getComponentLViewByIndex} from '../render3/util/view_utils';

import {RendererStyleFlags2, RendererType2} from './api_flags';


export const Renderer2Interceptor = new InjectionToken<Renderer2[]>('Renderer2Interceptor');


/**
 * Creates and initializes a custom renderer that implements the `Renderer2` base class.
 *
 * @publicApi
 */
export abstract class RendererFactory2 {
  /**
   * Creates and initializes a custom renderer for a host DOM element.
   * @param hostElement The element to render.
   * @param type The base class to implement.
   * @returns The new custom renderer instance.
   */
  abstract createRenderer(hostElement: any, type: RendererType2|null): Renderer2;
  /**
   * A callback invoked when rendering has begun.
   */
  abstract begin?(): void;
  /**
   * A callback invoked when rendering has completed.
   */
  abstract end?(): void;
  /**
   * Use with animations test-only mode. Notifies the test when rendering has completed.
   * @returns The asynchronous result of the developer-defined function.
   */
  abstract whenRenderingDone?(): Promise<any>;
}


/**
 * Extend this base class to implement custom rendering. By default, Angular
 * renders a template into DOM. You can use custom rendering to intercept
 * rendering calls, or to render to something other than DOM.
 *
 * Create your custom renderer using `RendererFactory2`.
 *
 * Use a custom renderer to bypass Angular's templating and
 * make custom UI changes that can't be expressed declaratively.
 * For example if you need to set a property or an attribute whose name is
 * not statically known, use the `setProperty()` or
 * `setAttribute()` method.
 *
 * @publicApi
 */
export abstract class Renderer2 {
"""
]